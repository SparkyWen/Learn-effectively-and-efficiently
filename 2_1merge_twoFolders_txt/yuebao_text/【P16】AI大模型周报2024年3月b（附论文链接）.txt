大家好,这里是AI大模型周报2024年3月的第二期。Anthropic推出Cloud3模型家族,包括Haiku,就是Cloud3最快最简洁的模型,几乎能够即时回复,还有Sonnet,就是面向企业客户的模型,以及Opus。Opus是Cloud3的极限模型,具备分析、预测、内容创作、精准总结、处理科学问题、多种对话等能力。在通用推理能力基准MMLU、数学能力基准MATH、还有Python编程能力基准Humanivow等的表现超越GPT-4。纽约大学和Meta的研究者推出开放知识机器人系统OK Robots,融合多个基于公开数据训练的模型,能够遵循自然语言指令在真实世界的环境中抓取和摆放物体。实验显示在杂乱的新场景中,OK Robots的任务成功率达到58.5%,在干净整洁的环境中成功率达到82.4%。康奈尔大学的研究者推出烹饪助手Mosaic,用户可以通过自然语言控制一个桌面机器人在一个移动机器人帮助准备蔬菜汤三明治等食物。首先由Interactive Task Planner,用户自然语言和机器人沟通决定做什么菜,然后为每个机器人派发子任务。Human Motion Forecasting将人类动作的2D图像转化成3D坐标,预测人类的下一步行动。同时,视觉语言模型,也就是VLM,Visual Language Model,将图像和语言作为输入,针对相关物体生成3D抓握姿势。最后由Visual Loader Scale模块来整合机器人执行任务。在六个菜谱的实验中,Mosaic的协作烹饪完成率达到68.3%,子任务完成率达到91.6%。阿里巴巴的研究者推出Emo,这是一个音频驱动的肖像视频生成框架,输入一张参考图和一段说话或者唱歌的音频,Emo可以让图像中的人物演绎这段音频,配合对应的口型以及丰富的面部表情和角度,支持不同风格的图像和任意时长的音频。相较Wave2Lip,DreamTalk等类似方法,Emo的表现更加突出。以色列初创公司Lighttricks推出生成式AI视频故事平台LTX Studio,输入文本可以生成时长25秒的微电影视频,提供故事板工具和场景规划工具,支持调整人物、场景、镜头、灯光等等,也支持自动编辑。平台计划在3月27日正式发布,目前已经开放预约。浙江大学的研究者推出视频编辑框架InuitEdit,支持编辑视频中的主体动作,比如让科基散步变成科基跑步,或者让弹其他的小浣熊变成吃苹果的小浣熊招手的小浣熊。InuitEdit也支持修改视频内容,包括主体对象、画面背景、艺术风格等等。相较JokenFlow,FakeZero等视频编辑方法,InuitEdit表现更优。苹果的研究者推出大模型赋能的动画设计工具Keyframer,能够基于静态图像生成动画,支持添加提示词或者编辑大模型生成的CSS动画代码和属性来进行设计迭代。提示类型包括修改动作、速度、位置、颜色,修改物体之间的关系,请大模型提供新的想法等等。复旦大学的研究者推出多模态大模型AIGPT,能够统一处理语音文本图像和音乐等各种模态组合,比如按照用户的语言指令创作一首关于春天的诗并用克隆的声音朗读出来。根据一段音乐生成一张图像,或者传输对图像的理解并生成相符的音乐等等。AIGPT只需要数据预处理和后处理,无需变更模型的架构和训练目标。北京大学的研究者针对大模型的语言学习能力研究能否让大模型仅凭提示prompting就学会一门新语言,团队收集了一套壮语研究材料,提出DIPMT++这个框架,让大模型通过语境学习适应新的语言,通过使用一本词典和5000组例句就实现了出色的汉语和壮语之间的互译。相关研究有助于保护濒危语言,保护语言多样性。顺带说一下,DIPMT代表Dictionary-Based Prompting for Machine Translation,面向机器翻译的基于词典的提示,就是MetaAI在2023年2月提出的一种机器学习翻译方法。北京大学的研究者面向城市规划推出PlanGPT,利用定制的本地数据库检索框架和专门领域微调的基础模型,负能城市空间规划工作,包括生成规划文本、检索相关信息、还有评估规划文件等等。亚马逊AWSAI的研究者面向生物医药领域推出BioBridge学习框架,旨在连接独立训练的基础模型,比如单纯预测蛋白质序列的模型,单纯研究小分子结构的模型,还有单纯面向医疗数据的模型等等。将这些独立的基础模型连接起来,BioBridge通过利用知识图谱、Knowledge Graphs,学习在没有微调的情况下将一个基础模型转移到另一个基础模型,从而支持多样的任务,尤其是任务跨模态检索任务,有望助能新药研发。苏黎世大学的研究者推出宣传手段检测工具Clarify,作为浏览器插件自动分析网页文本内容,有不同的检测程度选项,包括高光标注使用了宣传手段的部分,还有给出具体解释的版本。专家实验显示Clarify检测宣传手段的正确率较高,给出的解释比较清晰可信。顺海AI实验室的研究者面向开放世界的机器人操作推出Robot Script,旨在部署能够生成机器人控制代码的大语言模型并进行评估。输入层包括传感器输入、人类指令和机器人URDF统一描述格式数据。通过感知工具和行动规划工具的处理,通过大语言模型生成Python任务脚本,融合ROS等机器人框架,形成从解读语言到执行规划的端到端系统,也就是从Language Interpretation到Plan Execution。这样的机制有助于提高机器人应用的灵活度,更容易适应新的代码生成方式或者机器人架构。2023年10月,Microsoft Research中国科学技术大学以及清华大学的研究者面向AI训练的效率和成本问题推出量化方法BitNet。这是最早支持训练1bit大语言模型的网络结构,能够在保证性能的同时大幅降低大模型的训练和推理成本。2024年2月,Microsoft Research和国科大的研究者又推出BitNet的变体BitNet 1.58bit,在原来的1bit上添加了一个附加值0,让模型的每个参数都是三元的负1,01,得到二进制系统中的1.58bit。相较同等规模的全精度Transformer大模型,BitNet 1.58bit这个变体能够在维持性能的同时明显更加高效地改善延迟、内存、吞吐量和能源消耗问题。未来新的计算方式或开启专门面向1bit大模型设计的硬件。清华和哈工大的研究者推出量化感知训练框架1bit,将模型参数压缩到一个bit,提升模型推理的时间和空间效率,让在个人电脑和智能手机上部署大模型成为可能。实验显示1bit支持参数规模13亿到130亿的OPT、LAMA等模型,具备普遍适用性。这期节目就到这里,Thanks for listening!