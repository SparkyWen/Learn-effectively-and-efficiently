大家好,这里是AI大模型周报2024年3月的第三期。主创公司可能是AI推出Devon,号称第一个AI软件工程师,能够根据提示词规划和执行复杂的软件工程任务,包括创建网页,开发软件,代码编辑与纠错,训练微调自己的AI模型,学习使用不熟悉的技术等等。它能够做出数以千计的决策,从自己的错误中学习,迭代优化。团队还为Devon配备了代码编辑器等常用的开发者工具。另外它可以实时报告进展,接受反馈,让Devon像人类软件工程师那样工作并与人类合作。基于SWEBench的评估显示,Devon的表现远超GPT-4,SWELAMA,Cloud2等人气大模型。百度智能代码助手百度Commit新增Commit Plus开放平台和Otherwork私人研发助理能力,旨在满足企业的定制化开发需求,低成本打造智能代码助手。Commit Plus开放平台能够深度融合企业私域知识,第三方能力与编程现场,让代码助手适配企业的业务知识,团队规范还有标准流程以及研发管理。而Otherwork能够深入理解本地代码库和组织内部知识,开发者只需明确开发目标和意图,Otherwork就能自动检索背景知识,独立分析产品需求,匹配最佳解决方案并生成代码。Illinois大学和VMware Research的研究者推出SynCode框架,旨在高效使用大模型对代码进行语法解码。SynCode的主要创新在于DFA Mask Store,DFA是确定有限自动机,是能实现状态转移的自动机,让大模型生成的代码准确遵循目标编程语言的句法规则,从而连接理论上的模型能力与实际上的代码精准度,跨越多种编程语言生成代码。蒙特利尔大学和VMware Research的研究者探索大模型在剧本杀领域的应用,推出包括任务脚本、游戏规则在内的专门数据集,同时推出多代理交互框架ThinkThrice,取自三思而后行的三思ThinkThrice。在这里基于大模型的AI代理自主参与游戏,分别扮演玩家和主持人,向谁问问题、怎么回答其他角色的问题等所有的代理行动,都由大模型基于角色脚本和历史聊天记录生成。另外团队还如何上下文学习的最新进展,提高代理收集信息、逻辑推理、锁定凶手的能力。蒙特利尔大学等研究者面向网页端代理推出WorkArena基准,包含29项任务,旨在评测基于大模型的代理通过网页浏览器与企业软件互动并执行日常工作任务的能力。研究发现虽然现在的代理表现出潜力,但是和任务的完全自动化还有很大的距离。另外开源和闭源大模型的性能差异显著。继去年7月推出全球第一个控制机器人的VLA模型RT2之后,Google Daymind就在今年3月推出性能更优的RTH,能够将复杂的任务分解成具体的语言指令,比如任务是盖上开心果瓶子的盖子,RTH就会预测语言动作Language Motions,比如向右旋转机械臂,这样创建动作优先级,也是Action Hierarchy,可以在任务的执行过程中加入人类干涉,纠正动作语言,防止任务失败。RTH会从这些干涉中学习,并且可以在不同的任务之间学习共同的动作结构,更好地跨越多任务数据集共享数据。机器人初创公司Covariant推出RFM1机器人基础模型,堪称机器人ChatGPT,这是一个理解物理学的世界模型,能够准确地模拟和预测真实世界中机器人的操作场景,可以理解并执行自然语言指令。RFM1的训练基于互联网上数据以及Covariant公司自2007年开发的世界最大的多模态机器人数据集。模型目前主要支持仓库操作类任务,未来有望进入任何巨身,包括人形机器人。我们在上期节目有详细的官方介绍中文版,感兴趣的同学请参考。中国广核集团福建宁德核电有限公司发布大模型警书,专为核工业领域打造,参数规模720亿,旨在探索利用AI大模型解决核电行业面临的各种挑战,比如知识管理不足、低脑力劳动力过多、安全分析能力有待增强等等。华南理工大学和字节跳动AI Lab的研究者面向股票收益预测,创建自动化系统FinReport,旨在帮助投资者收集信息并进行分析,生成相关资讯的总结报告。FinReport由三个模块组成,分别负责新闻整理、回报预测以及风险评估。其中回报预测模块会分析股票相关新闻可能对市场情绪造成的影响。基于真实世界数据集的实验证明,FinReport具备有效性和可解释性。朱蒂新加坡的Brilliant Labs推出新产品Frame,这款轻量型AR眼镜有多模态AI助手Nova赋能,融合一系列AI模型,包括Perplexity AI的对话式搜索引擎,Stability AI的文生成图模型Stable Diffusion,OpenAI的GPT-4以及语音识别系统Whisper。Frame能够进行视觉处理、图像生成、翻译等任务,芯片的图像分辨率达到640x400。由天图万境团队开发与华为云合作部署的视频声音频通用工具Sora Opera将在3月上线并面向公众开放,用于在文本生成视频的静音画面添加音效或者背景音乐,产品首先在华为云平台上发布。复旦大学和腾讯PCG的研究者面向长视频理解推出Goofy LLM,旨在为长视频创建高质量合成数据,框架利用GPT-4和文生成图模型为长视频生成连贯的关键画面以及对应的问答脚本,能够显著提升多模态大模型的长视频理解能力。Meta的研究者面向电商、社交等数字平台的大规模推送系统推出悟空,包括基于对敌式因子分解机的高效网络架构以及协同扩展策略,在推荐领域建立缩放法则,从而持续提高模型质量,适应更加复杂的真实世界数据集。并且发现悟空在各种复杂度级别上优于所有基线,并且随着参数放大,模型性能更加稳定。UC Berkeley的研究者对语言模型的预测能力进行研究,开发了一个检索增强的语言模型系统,自动搜索相关信息,生成预测,并与人类的预测做比较,并发现该系统的表现接近人类的预测水平,在一些场景下甚至有过之无不及。研究者认为可以使用语言模型预测未来辅助机构决策。伦敦大学、剑桥大学、牛津大学等研究者创建Brain Bench基准,基于神经科学文献微调,测试发现,在预测实验结果方面,大模型的表现超越人类专家。该研究方法不仅限于神经科学领域,能够扩展到其他知识密集性领域。波士顿大学的研究者为推荐大模型的部署落地推出混合方案,结合不同规模的语言模型,由小语言模型SLM高效生成回复,在保持性能的同时提高自回归解码效率。实验发现LLM2SLM方案可为翻译和总结任务最高提速4倍,性能代偿1%到2%。华为联合复旦大学等在Nature Communications发表文章《储存计算的未来新机遇和挑战》,旨在阐明储存计算的数学理论、算法设计和实验实现的并行进展,讨论大规模工业采用储存计算的机遇和挑战。相较需要训练大型神经网络模型的深度学习,神经形态计算关注开发新型的低能耗计算系统,从脑科学汲取灵感,来为信息处理创造节能硬件并适应高度复杂的任务。储存计算或者说储备池计算,而Reservoir Computing它作为神经形态计算的一个重要的模型家族,在过去20多年取得了重大进展,具备训练快、能耗低、实时处理、动态可适应等优点,在音色数据中心、智能机器人、数字孪生、6G网络、新一代光纤网络、物联网等领域应用前景广阔。苹果机器学习研究中心Apple Machine Learning Research公布2024年AI ML也就是人工智能和机器学习领域获得博士生奖学金的苹果学者名单,共有21位年轻学者获得了苹果学者计划的资助,华人比重超过一半。这期节目就到这里,前面提到的论文链接请参考视频下方的简介部分。Thanks for listening!